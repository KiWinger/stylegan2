{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Edit_new_image.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedro3/stylegan2/blob/master/Edit_new_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VWtBaGbmLubM"
      },
      "source": [
        "# Edit_new_image\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7bAVpVONtwUw"
      },
      "source": [
        "# 1.Set up\n",
        "指定環境である **tensorflow 1.14** に変更し、それに伴い **keras 2.0.8** に変更します。そして、githubからコードをダウンロードします。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lyI-JqcYsmbT",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.14\n",
        "!pip install tensorflow-gpu==1.14\n",
        "!pip install keras==2.0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Bnd_uPRo2Co",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/cedro3/stylegan2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OCGJGVrUpnQY",
        "colab": {}
      },
      "source": [
        "cd stylegan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zFJNpgfw79f4"
      },
      "source": [
        "#2.Define Function\n",
        "コードを実行すると、必要なクラスと関数を定義します。\\\n",
        "＊新規画像を作成するクラスと関数も一緒に定義しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UwU_4rN5SgZR",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import PIL.Image\n",
        "import sys\n",
        "import bz2\n",
        "from keras.utils import get_file\n",
        "import dlib\n",
        "import argparse\n",
        "import numpy as np\n",
        "import dnnlib\n",
        "import dnnlib.tflib as tflib\n",
        "import re\n",
        "import projector\n",
        "import pretrained_networks\n",
        "from training import dataset\n",
        "from training import misc\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange\n",
        "\n",
        "class LandmarksDetector:\n",
        "    def __init__(self, predictor_model_path):\n",
        "        \"\"\"\n",
        "        :param predictor_model_path: path to shape_predictor_68_face_landmarks.dat file\n",
        "        \"\"\"\n",
        "        self.detector = dlib.get_frontal_face_detector() # cnn_face_detection_model_v1 also can be used\n",
        "        self.shape_predictor = dlib.shape_predictor(predictor_model_path)\n",
        "\n",
        "    def get_landmarks(self, image):\n",
        "        img = dlib.load_rgb_image(image)\n",
        "        dets = self.detector(img, 1)\n",
        "\n",
        "        for detection in dets:\n",
        "            face_landmarks = [(item.x, item.y) for item in self.shape_predictor(img, detection).parts()]\n",
        "            yield face_landmarks\n",
        "\n",
        "def image_align(src_file, dst_file, face_landmarks, output_size=1024, transform_size=4096, enable_padding=True):\n",
        "        # Align function from FFHQ dataset pre-processing step\n",
        "        # https://github.com/NVlabs/ffhq-dataset/blob/master/download_ffhq.py\n",
        "        src_file\n",
        "        dst_file \n",
        "        face_landmarks\n",
        "        output_size=1024 \n",
        "        transform_size=4096 \n",
        "        enable_padding=True\n",
        "            \n",
        "        lm = np.array(face_landmarks)\n",
        "        lm_chin          = lm[0  : 17]  # left-right\n",
        "        lm_eyebrow_left  = lm[17 : 22]  # left-right\n",
        "        lm_eyebrow_right = lm[22 : 27]  # left-right\n",
        "        lm_nose          = lm[27 : 31]  # top-down\n",
        "        lm_nostrils      = lm[31 : 36]  # top-down\n",
        "        lm_eye_left      = lm[36 : 42]  # left-clockwise\n",
        "        lm_eye_right     = lm[42 : 48]  # left-clockwise\n",
        "        lm_mouth_outer   = lm[48 : 60]  # left-clockwise\n",
        "        lm_mouth_inner   = lm[60 : 68]  # left-clockwise\n",
        "\n",
        "        # Calculate auxiliary vectors.\n",
        "        eye_left     = np.mean(lm_eye_left, axis=0)\n",
        "        eye_right    = np.mean(lm_eye_right, axis=0)\n",
        "        eye_avg      = (eye_left + eye_right) * 0.5\n",
        "        eye_to_eye   = eye_right - eye_left\n",
        "        mouth_left   = lm_mouth_outer[0]\n",
        "        mouth_right  = lm_mouth_outer[6]\n",
        "        mouth_avg    = (mouth_left + mouth_right) * 0.5\n",
        "        eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "        # Choose oriented crop rectangle.\n",
        "        x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "        x /= np.hypot(*x)\n",
        "        x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "        y = np.flipud(x) * [-1, 1]\n",
        "        c = eye_avg + eye_to_mouth * 0.1\n",
        "        quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "        qsize = np.hypot(*x) * 2\n",
        "\n",
        "        # Load in-the-wild image.\n",
        "        if not os.path.isfile(src_file):\n",
        "            print('\\nCannot find source image. Please run \"--wilds\" before \"--align\".')\n",
        "            return\n",
        "        img = PIL.Image.open(src_file)\n",
        "\n",
        "        # Shrink.\n",
        "        shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "        if shrink > 1:\n",
        "            rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "            img = img.resize(rsize, PIL.Image.ANTIALIAS)\n",
        "            quad /= shrink\n",
        "            qsize /= shrink\n",
        "\n",
        "        # Crop.\n",
        "        border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "        crop = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]), min(crop[3] + border, img.size[1]))\n",
        "        if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "            img = img.crop(crop)\n",
        "            quad -= crop[0:2]\n",
        "\n",
        "        # Pad.\n",
        "        pad = (int(np.floor(min(quad[:,0]))), int(np.floor(min(quad[:,1]))), int(np.ceil(max(quad[:,0]))), int(np.ceil(max(quad[:,1]))))\n",
        "        pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0), max(pad[3] - img.size[1] + border, 0))\n",
        "        if enable_padding and max(pad) > border - 4:\n",
        "            pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "            img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "            h, w, _ = img.shape\n",
        "            y, x, _ = np.ogrid[:h, :w, :1]\n",
        "            mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w-1-x) / pad[2]), 1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h-1-y) / pad[3]))\n",
        "            blur = qsize * 0.02\n",
        "            img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "            img += (np.median(img, axis=(0,1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "            img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "            quad += pad[:2]\n",
        "\n",
        "        # Transform.\n",
        "        img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "        if output_size < transform_size:\n",
        "            img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "        # Save aligned image.\n",
        "        img.save(dst_file, 'PNG')\n",
        "\n",
        "\n",
        "def unpack_bz2(src_path):\n",
        "    data = bz2.BZ2File(src_path).read()\n",
        "    dst_path = src_path[:-4]\n",
        "    with open(dst_path, 'wb') as fp:\n",
        "        fp.write(data)\n",
        "    return dst_path\n",
        "\n",
        "\n",
        "def project_real_images(): \n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n",
        "    dataset_name = 'dataset'  \n",
        "    data_dir = 'my'  \n",
        "    num_images =  6 \n",
        "    num_snapshots = 5\n",
        "\n",
        "    print('Loading networks from \"%s\"...' % network_pkl)\n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    proj = projector.Projector()\n",
        "    proj.set_network(Gs)\n",
        "\n",
        "    print('Loading images from \"%s\"...' % dataset_name)\n",
        "    dataset_obj = dataset.load_dataset(data_dir=data_dir, tfrecord_dir=dataset_name, max_label_size=0, repeat=False, shuffle_mb=0)\n",
        "    assert dataset_obj.shape == Gs.output_shape[1:]\n",
        "\n",
        "    os.makedirs('my/real_images', exist_ok=True)  \n",
        "    for image_idx in range(num_images):\n",
        "        print('Projecting image %d/%d ...' % (image_idx, num_images))\n",
        "        images, _labels = dataset_obj.get_minibatch_np(1)\n",
        "        images = misc.adjust_dynamic_range(images, [0, 255], [-1, 1])\n",
        "        \n",
        "        targets=images\n",
        "        png_prefix='./my/real_images/image'+str(image_idx)\n",
        "        num_snapshots=num_snapshots\n",
        "                \n",
        "        snapshot_steps = set(proj.num_steps - np.linspace(0, proj.num_steps, num_snapshots, endpoint=False, dtype=int))\n",
        "        misc.save_image_grid(targets, png_prefix + 'target.png', drange=[-1,1])\n",
        "        proj.start(targets)\n",
        "        while proj.get_cur_step() < proj.num_steps:\n",
        "            print('\\r%d / %d ... ' % (proj.get_cur_step(), proj.num_steps), end='', flush=True)\n",
        "            proj.step()\n",
        "            if proj.get_cur_step() in snapshot_steps:\n",
        "                misc.save_image_grid(proj.get_images(), png_prefix + 'step%04d.png' % proj.get_cur_step(), drange=[-1,1])\n",
        "            \n",
        "            if proj.get_cur_step() == proj.num_steps:  \n",
        "                vec = proj.get_dlatents() \n",
        "                if image_idx == 0:\n",
        "                   vec_syn = vec\n",
        "                else:\n",
        "                   vec_syn = np.concatenate([vec_syn, vec])  \n",
        "                print(vec_syn.shape)  \n",
        "        print('\\r%-30s\\r' % '', end='', flush=True)\n",
        "\n",
        "    return vec_syn\n",
        "\n",
        "def generate_real_gif(vec_syn):\n",
        "    vec0 = vec_syn[0].reshape(1, 18, 512)\n",
        "    vec1 = vec_syn[1].reshape(1, 18, 512)\n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'  \n",
        "    \n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = True \n",
        "    Gs_syn_kwargs.truncation_psi = 0.5\n",
        "\n",
        "    image_gif = []\n",
        "    image_gif_512 = []\n",
        "    image_gif_256 = []\n",
        "    os.makedirs('my/real_gif', exist_ok=True)    \n",
        "    for i in range(40):\n",
        "        vec = vec0+(vec1-vec0)*i/39\n",
        "        images =  Gs.components.synthesis.run(vec, **Gs_syn_kwargs) \n",
        "        image_one = PIL.Image.fromarray(images[0], 'RGB')\n",
        "        image_gif.append(image_one)\n",
        "        image_gif_512.append(image_one.resize((512,512))) \n",
        "        image_gif_256.append(image_one.resize((256,256))) \n",
        "\n",
        "    image_gif[0].save('./my/real_gif/movie.gif', save_all=True, append_images=image_gif[1:],\n",
        "                      duration=100, loop=0)\n",
        "    image_gif_512[0].save('./my/real_gif/movie_512.gif', save_all=True, append_images=image_gif_512[1:],\n",
        "                      duration=100, loop=0)    \n",
        "    image_gif_256[0].save('./my/real_gif/movie_256.gif', save_all=True, append_images=image_gif_256[1:],\n",
        "                      duration=100, loop=0)\n",
        "    print('complete !')\n",
        "\n",
        "\n",
        "def generate_images_from_style(all_w):\n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'  \n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = True\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 40))\n",
        "    os.makedirs('my/generate_pic', exist_ok=True)    \n",
        "    for i in trange(len(all_w)):\n",
        "        images = Gs.components.synthesis.run(all_w, **Gs_syn_kwargs)\n",
        "        PIL.Image.fromarray(images[i], 'RGB').save('./my/generate_pic/'+str(i)+'.png')     \n",
        " \n",
        "        ax = fig.add_subplot(10, 10, i+1, xticks=[], yticks=[])\n",
        "        image_plt = np.array(images[i])\n",
        "        ax.imshow(image_plt)\n",
        "        ax.set_xlabel(str(i), fontsize=20)               \n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def style_mixing(vec_syn, col_styles, truncation_psi):  \n",
        "    network_pkl = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n",
        "    row_seeds = [1, 2, 3]\n",
        "    col_seeds = [4, 5, 6]\n",
        "    col_styles = col_styles\n",
        "    truncation_psi = truncation_psi\n",
        "    minibatch_size = 4\n",
        "\n",
        "    # Loading networks \n",
        "    _G, _D, Gs = pretrained_networks.load_networks(network_pkl)\n",
        "\n",
        "    Gs_syn_kwargs = dnnlib.EasyDict()\n",
        "    Gs_syn_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
        "    Gs_syn_kwargs.randomize_noise = False\n",
        "    Gs_syn_kwargs.minibatch_size = minibatch_size\n",
        "\n",
        "    # Generating W vectors\n",
        "    all_seeds = list(row_seeds + col_seeds)   \n",
        "    all_w = vec_syn[:6]\n",
        "    w_dict = {seed: w for seed, w in zip(all_seeds, list(all_w))} # [layer, component]\n",
        "\n",
        "    print('Generating images...') \n",
        "    all_images = Gs.components.synthesis.run(all_w, **Gs_syn_kwargs) # [minibatch, height, width, channel]\n",
        "    image_dict = {(seed, seed): image for seed, image in zip(all_seeds, list(all_images))}\n",
        "\n",
        "    print('Generating style-mixed images...')\n",
        "    for row_seed in row_seeds:\n",
        "        for col_seed in col_seeds:\n",
        "            w = w_dict[row_seed].copy()\n",
        "            w[col_styles] = w[col_styles] + (w_dict[col_seed][col_styles]-w[col_styles])*truncation_psi\n",
        "            image = Gs.components.synthesis.run(w[np.newaxis], **Gs_syn_kwargs)[0]\n",
        "            image_dict[(row_seed, col_seed)] = image\n",
        "\n",
        "    print('Saving images...')\n",
        "    os.makedirs('my/stylemix_images', exist_ok=True)\n",
        "    for (row_seed, col_seed), image in image_dict.items():\n",
        "        PIL.Image.fromarray(image, 'RGB').save('./my/stylemix_images/'+str(row_seed)+'-'+str(col_seed)+'.png')\n",
        "\n",
        "    print('Saving image grid...')\n",
        "    _N, _C, H, W = Gs.output_shape\n",
        "    canvas = PIL.Image.new('RGB', (W * (len(col_seeds) + 1), H * (len(row_seeds) + 1)), 'black')\n",
        "\n",
        "    r, c = 4, 4  # スクリーン設定（4行×4列）\n",
        "    fig, axs = plt.subplots(r, c, figsize=(16,16), subplot_kw=({'xticks':(),'yticks':()}))\n",
        "\n",
        "    for row_idx, row_seed in enumerate([None] + row_seeds):\n",
        "        for col_idx, col_seed in enumerate([None] + col_seeds):\n",
        "            if row_seed is None and col_seed is None:\n",
        "                continue\n",
        "            key = (row_seed, col_seed)\n",
        "            if row_seed is None:\n",
        "                key = (col_seed, col_seed)\n",
        "            if col_seed is None:\n",
        "                key = (row_seed, row_seed)\n",
        "            canvas.paste(PIL.Image.fromarray(image_dict[key], 'RGB'), (W * col_idx, H * row_idx)) \n",
        "\n",
        "            # スクリーンに画像配置            \n",
        "            image_plt = np.array(image_dict[key])\n",
        "            axs[row_idx, col_idx].imshow(image_plt)\n",
        "            if row_seed is None:\n",
        "                x, y = col_seed, col_seed\n",
        "            elif col_seed is None:\n",
        "                x, y = row_seed, row_seed\n",
        "            else:\n",
        "                x, y = row_seed, col_seed\n",
        "            axs[row_idx, col_idx].set_xlabel(str(x)+'-'+str(y))\n",
        "\n",
        "    canvas.save('./my/stylemix_images/grid.png') \n",
        "\n",
        "    # スクリーン表示\n",
        "    black = np.zeros((1024,1024,3))  # 黒画像作成\n",
        "    axs[0,0].imshow(black)\n",
        "    axs[0,0].axis('off')\n",
        "    plt.show()\n",
        "    plt.close()   \n",
        "\n",
        "\n",
        "# make dirctory\n",
        "os.makedirs('my/img', exist_ok=True)\n",
        "os.makedirs('my/al_img', exist_ok=True)\n",
        "os.makedirs('my/pic', exist_ok=True)\n",
        "os.makedirs('my/vectors', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Efgp96_Led-R"
      },
      "source": [
        "#3.load_vectors\n",
        "　Sampleデータをロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gQfhcVIeelrQ",
        "colab": {}
      },
      "source": [
        "vec_direction = np.load('Sample/vectors/vec_direction.npy')\n",
        "vec_smile = np.load('Sample/vectors/vec_smile.npy')\n",
        "vec_glass = np.load('Sample/vectors/vec_glass.npy')\n",
        "vec_young = np.load('Sample/vectors/vec_young.npy')\n",
        "vec_old = np.load('Sample/vectors/vec_old.npy')\n",
        "vec_man = np.load('Sample/vectors/vec_man.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t_D7kUH6gyPB"
      },
      "source": [
        "#4.show_generate_images\n",
        "指定したベクトル(潜在変数w)から生成した画像を表示します。(初回実行時は時間が掛かりますが、２回目以降は早いです）\\\n",
        "＊生成した画像は、my/generate_picに上書き保存します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NYoKKRqtg6iB",
        "colab": {}
      },
      "source": [
        "generate_images_from_style(vec_direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "POm2EngYhGAl"
      },
      "source": [
        "#5.Style_Mixing\n",
        "指定された潜在変数wをRow_picとCol_picで入れ替えます。\\\n",
        "第１引数：ベクトル名\\\n",
        "第２引数：潜在変数wの指定（リスト形式）\\\n",
        "第３引数：Col_picの潜在変数の寄与度(0〜1はRow_colとの混合割合)\\\n",
        "＊第２引数、第３引数を色々変更して変化を見ると面白いです。\\\n",
        "＊生成した画像は、my/stylemix_images に上書き保存します\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60vVvvTNfrUI",
        "colab": {}
      },
      "source": [
        "# Face_direction\n",
        "style_mixing(vec_direction, [0,1], 1.4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l2OvM47BhYKS",
        "colab": {}
      },
      "source": [
        "# Smile\n",
        "style_mixing(vec_smile, [4,5], 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9t9jN22ZhOP-",
        "colab": {}
      },
      "source": [
        "# Glass\n",
        "style_mixing(vec_glass, [0,1,2], 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rWuvM5yuhb3r",
        "colab": {}
      },
      "source": [
        "# Young\n",
        "style_mixing(vec_young, [4,5,6,7], 0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M3eoleBhhcmx",
        "colab": {}
      },
      "source": [
        "# Old\n",
        "style_mixing(vec_old, [4,5,6,7], 0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGRpoGzRhgMX",
        "colab": {}
      },
      "source": [
        "# Man\n",
        "style_mixing(vec_man, [4,5,6,7], 1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kM_IWlwLiIfV"
      },
      "source": [
        "#Appendix\n",
        "# ------ 以下は自分で新規画像のベクトルデータを作成するためのもの ------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5kovbSW_8_kr"
      },
      "source": [
        "#10.Get target image from original image\n",
        "オリジナル画像を **my/imgフォルダーへアップロードしてから**、下記コードを実行する。\\\n",
        "切り出された顔画像は **my/al_imgフォルダー**へ保存される。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "96ZV6vhd0Cmn",
        "colab": {}
      },
      "source": [
        "LANDMARKS_MODEL_URL = 'http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2'\n",
        "landmarks_model_path = unpack_bz2(get_file('shape_predictor_68_face_landmarks.dat.bz2',\n",
        "                                            LANDMARKS_MODEL_URL, cache_subdir='temp'))\n",
        "RAW_IMAGES_DIR = 'my/img'\n",
        "ALIGNED_IMAGES_DIR = 'my/al_img'\n",
        "\n",
        "landmarks_detector = LandmarksDetector(landmarks_model_path)\n",
        "for img_name in os.listdir(RAW_IMAGES_DIR):\n",
        "    raw_img_path = os.path.join(RAW_IMAGES_DIR, img_name)\n",
        "    for i, face_landmarks in enumerate(landmarks_detector.get_landmarks(raw_img_path), start=1):\n",
        "        face_img_name = '%s_%02d.png' % (os.path.splitext(img_name)[0], i)\n",
        "        aligned_face_path = os.path.join(ALIGNED_IMAGES_DIR, face_img_name)\n",
        "        image_align(raw_img_path, aligned_face_path, face_landmarks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xLy3vbC-8OOf"
      },
      "source": [
        "#11.Make TFRecord dataset from target image\n",
        "潜在変数wを探索する画像を **my/al_imgフォルダー**から**６つ選択して my/picフォルダーへ\\\n",
        "ドラッグ＆ドロップしてから**、下記コードを実行する。\\\n",
        "実行後マルチ解像度のデータセットは **my/datasetフォルダー**に保存される。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pfc9qX-RjgKW",
        "colab": {}
      },
      "source": [
        "!python dataset_tool.py create_from_images ./my/dataset ./my/pic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fx3mTkCp-9o5"
      },
      "source": [
        "#12.Find the matching latent vector \n",
        "コードを実行するとmy/datasetフォルダーにあるマルチ解像度のデータセットから潜在変数wを探索する。\\\n",
        "途中経過の画像は **my/real_imagesフォルダー**に保存される。\\\n",
        "探索した６つの潜在変数は**vec_syn**で返って来る。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sWDyDSJECsBR",
        "colab": {}
      },
      "source": [
        "vec_syn = project_real_images()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hZuW4tRi938c"
      },
      "source": [
        "#13.Make animation from latent vector\n",
        "コードを実行すると、**変数vec_syn**に格納されている先頭の２つの潜在変数からアニメーションを作成します。\\\n",
        "作成した gifアニメーションは**my/real_gifフォルダー**に保存されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-QtFwM756gv",
        "colab": {}
      },
      "source": [
        "generate_real_gif(vec_syn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JojKyF7vZcj5"
      },
      "source": [
        "#14.Save vectors\n",
        "作成したベンクトルデータを保存します。\\\n",
        "＊保存先は、my/vectors です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cvjkx88A40Bq",
        "colab": {}
      },
      "source": [
        "np.save('my/vectors/vec_syn', vec_syn)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}